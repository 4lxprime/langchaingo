"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[320],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>f});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=r.createContext({}),s=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},m=function(e){var t=s(e.components);return r.createElement(c.Provider,{value:t},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},g=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),u=s(n),g=a,f=u["".concat(c,".").concat(g)]||u[g]||p[g]||o;return n?r.createElement(f,l(l({ref:t},m),{},{components:n})):r.createElement(f,l({ref:t},m))}));function f(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,l=new Array(o);l[0]=g;var i={};for(var c in t)hasOwnProperty.call(t,c)&&(i[c]=t[c]);i.originalType=e,i[u]="string"==typeof e?e:a,l[1]=i;for(var s=2;s<o;s++)l[s]=n[s];return r.createElement.apply(null,l)}return r.createElement.apply(null,n)}g.displayName="MDXCreateElement"},4874:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>m,contentTitle:()=>c,default:()=>f,frontMatter:()=>i,metadata:()=>s,toc:()=>u});n(7294);var r=n(3905);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){return t=null!=t?t:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):function(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})),e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}const i={sidebar_position:1},c="Quickstart: LangChainGo with Ollama",s={unversionedId:"getting-started/guide-ollama",id:"getting-started/guide-ollama",title:"Quickstart: LangChainGo with Ollama",description:"Get started with running your first program using LangChainGo and Ollama. Ollama provides the most straightforward method for local LLM inference across all computer platforms.",source:"@site/docs/getting-started/guide-ollama.mdx",sourceDirName:"getting-started",slug:"/getting-started/guide-ollama",permalink:"/langchaingo/docs/getting-started/guide-ollama",draft:!1,editUrl:"https://github.com/tmc/langchaingo/edit/main/docs/docs/getting-started/guide-ollama.mdx",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"sidebar",previous:{title:"Welcome to LangChainGo",permalink:"/langchaingo/docs/"},next:{title:"Quickstart: LangChainGo with OpenAI",permalink:"/langchaingo/docs/getting-started/guide-openai"}},m={},u=[{value:"Pre-requisites",id:"pre-requisites",level:2},{value:"Steps",id:"steps",level:2}],p={toc:u},g="wrapper";function f(e){var{components:t}=e,n=l(e,["components"]);return(0,r.kt)(g,o(function(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{},r=Object.keys(n);"function"==typeof Object.getOwnPropertySymbols&&(r=r.concat(Object.getOwnPropertySymbols(n).filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable})))),r.forEach((function(t){a(e,t,n[t])}))}return e}({},p,n),{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"quickstart-langchaingo-with-ollama"},"Quickstart: LangChainGo with Ollama"),(0,r.kt)("p",null,"Get started with running your first program using LangChainGo and ",(0,r.kt)("a",{parentName:"p",href:"https://ollama.ai/"},"Ollama"),". Ollama provides the most straightforward method for local LLM inference across all computer platforms."),(0,r.kt)("h2",{id:"pre-requisites"},"Pre-requisites"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Ollama"),": ",(0,r.kt)("a",{parentName:"li",href:"https://ollama.ai/"},"Download and install Ollama"),"."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Go"),": ",(0,r.kt)("a",{parentName:"li",href:"https://go.dev/doc/install"},"Download and install Go"),".")),(0,r.kt)("h2",{id:"steps"},"Steps"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Initialize Ollama"),": In your terminal, execute the command ",(0,r.kt)("inlineCode",{parentName:"li"},"$ ollama run llama2"),". The first run might take some time as the model needs to be fetched to your computer."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Run the Example"),": Enter the command:",(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"go run github.com/tmc/langchaingo/examples/ollama-completion-example@main\n")))),(0,r.kt)("p",null,"You should receive (something like) the following output:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"The first human to set foot on the moon was Neil Armstrong, an American astronaut, who stepped onto the lunar surface during the Apollo 11 mission on July 20, 1969.\n")),(0,r.kt)("p",null,"Congratulations! You've successfully built and executed your first open-source LLM-based program using local inference."),(0,r.kt)("p",null,"Here is the entire program (from ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/tmc/langchaingo/blob/main/examples/ollama-completion-example/ollama_completion_example.go"},"ollama-completion-example"),")."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-go"},'package main\n\nimport (\n    "context"\n    "fmt"\n    "log"\n\n    "github.com/tmc/langchaingo/llms"\n    "github.com/tmc/langchaingo/llms/ollama"\n)\n\nfunc main() {\n    llm, err := ollama.New(ollama.WithModel("llama2"))\n    if err != nil {\n        log.Fatal(err)\n    }\n    ctx := context.Background()\n    completion, err := llm.Call(ctx, "Human: Who was the first man to walk on the moon?\\nAssistant:",\n        llms.WithTemperature(0.8),\n        llms.WithStreamingFunc(func(ctx context.Context, chunk []byte) error {\n            fmt.Print(string(chunk))\n            return nil\n        }),\n    )\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    _ = completion\n}\n')))}f.isMDXComponent=!0}}]);