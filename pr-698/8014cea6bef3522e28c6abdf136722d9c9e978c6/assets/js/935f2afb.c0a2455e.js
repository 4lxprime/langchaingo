"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"sidebar":[{"type":"link","label":"Welcome to LangChainGo","href":"/langchaingo/docs/","docId":"index"},{"type":"category","label":"Getting Started","collapsed":false,"collapsible":false,"items":[{"type":"link","label":"Quickstart: LangChainGo with Ollama","href":"/langchaingo/docs/getting-started/guide-ollama","docId":"getting-started/guide-ollama"},{"type":"link","label":"Quickstart: LangChainGo with OpenAI","href":"/langchaingo/docs/getting-started/guide-openai","docId":"getting-started/guide-openai"}]},{"type":"category","label":"Components","collapsed":false,"collapsible":false,"items":[{"type":"category","label":"agents","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Agents","href":"/langchaingo/docs/modules/agents/agents/","docId":"modules/agents/agents/index"},{"type":"category","label":"Agent Executors","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Getting Started","href":"/langchaingo/docs/modules/agents/executor/getting-started","docId":"modules/agents/executor/getting-started"}],"href":"/langchaingo/docs/modules/agents/executor/"},{"type":"link","label":"Tools","href":"/langchaingo/docs/modules/agents/tools/","docId":"modules/agents/tools/index"}]},{"type":"category","label":"data_connection","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Text Splitters","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Examples","href":"/langchaingo/docs/modules/data_connection/text_splitters/examples/","docId":"modules/data_connection/text_splitters/examples/index"}],"href":"/langchaingo/docs/modules/data_connection/text_splitters/"},{"type":"link","label":"Retrievers","href":"/langchaingo/docs/modules/data_connection/retrievers/","docId":"modules/data_connection/retrievers/index"}]},{"type":"category","label":"memory","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Examples","href":"/langchaingo/docs/modules/memory/examples/","docId":"modules/memory/examples/index"}]},{"type":"category","label":"model_io","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Prompts","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Prompt Templates","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Partial Values","href":"/langchaingo/docs/modules/model_io/prompts/prompt_templates/partial_values","docId":"modules/model_io/prompts/prompt_templates/partial_values"}],"href":"/langchaingo/docs/modules/model_io/prompts/prompt_templates/"}],"href":"/langchaingo/docs/modules/model_io/prompts/"},{"type":"category","label":"Models","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"LLMs","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Integrations","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Hugging Face","href":"/langchaingo/docs/modules/model_io/models/llms/Integrations/huggingface","docId":"modules/model_io/models/llms/Integrations/huggingface"},{"type":"link","label":"Local","href":"/langchaingo/docs/modules/model_io/models/llms/Integrations/local","docId":"modules/model_io/models/llms/Integrations/local"},{"type":"link","label":"OpenAI","href":"/langchaingo/docs/modules/model_io/models/llms/Integrations/openai","docId":"modules/model_io/models/llms/Integrations/openai"},{"type":"link","label":"Vertex AI","href":"/langchaingo/docs/modules/model_io/models/llms/Integrations/vertexai","docId":"modules/model_io/models/llms/Integrations/vertexai"}]}],"href":"/langchaingo/docs/modules/model_io/models/llms/"},{"type":"category","label":"Chat Models","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Integrations","href":"/langchaingo/docs/modules/model_io/models/chat/integrations","docId":"modules/model_io/models/chat/integrations"}],"href":"/langchaingo/docs/modules/model_io/models/chat/"},{"type":"category","label":"Embeddings","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Integrations","href":"/langchaingo/docs/modules/model_io/models/embeddings/integrations","docId":"modules/model_io/models/embeddings/integrations"}],"href":"/langchaingo/docs/modules/model_io/models/embeddings/"}],"href":"/langchaingo/docs/modules/model_io/models/"},{"type":"link","label":"Output Parsers","href":"/langchaingo/docs/modules/model_io/output_parsers/","docId":"modules/model_io/output_parsers/index"}]}]},{"type":"link","label":"API reference","href":"https://pkg.go.dev/github.com/tmc/langchaingo"}]},"docs":{"getting-started/guide-ollama":{"id":"getting-started/guide-ollama","title":"Quickstart: LangChainGo with Ollama","description":"Get started with running your first program using LangChainGo and Ollama. Ollama provides the most straightforward method for local LLM inference across all computer platforms.","sidebar":"sidebar"},"getting-started/guide-openai":{"id":"getting-started/guide-openai","title":"Quickstart: LangChainGo with OpenAI","description":"Dive right into executing your first program utilizing LangChainGo in tandem with OpenAI. OpenAI\'s GPT models are renowned for their proficiency and expansive capabilities.","sidebar":"sidebar"},"index":{"id":"index","title":"Welcome to LangChainGo","description":"gopher","sidebar":"sidebar"},"modules/agents/agents/index":{"id":"modules/agents/agents/index","title":"Agents","description":"Conceptual Guide","sidebar":"sidebar"},"modules/agents/executor/getting-started":{"id":"modules/agents/executor/getting-started","title":"Getting Started: Agent Executors","description":"Agents use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.","sidebar":"sidebar"},"modules/agents/executor/index":{"id":"modules/agents/executor/index","title":"Agent Executors","description":"Conceptual Guide","sidebar":"sidebar"},"modules/agents/tools/index":{"id":"modules/agents/tools/index","title":"Tools","description":"Conceptual Guide","sidebar":"sidebar"},"modules/data_connection/retrievers/index":{"id":"modules/data_connection/retrievers/index","title":"Retrievers","description":"Conceptual Guide","sidebar":"sidebar"},"modules/data_connection/text_splitters/examples/index":{"id":"modules/data_connection/text_splitters/examples/index","title":"Text Splitters: Examples","description":"Splitters are components or tools used to divide texts into smaller, more manageable parts or specific segments. This division can be necessary for various reasons, such as improving the processing, analysis, or understanding of large or complex texts. Splitters can be simple, like dividing a text into sentences or paragraphs, or more complex, such as splitting based on themes, topics, or specific grammatical structures.","sidebar":"sidebar"},"modules/data_connection/text_splitters/index":{"id":"modules/data_connection/text_splitters/index","title":"Getting Started: Text Splitters","description":"Conceptual Guide","sidebar":"sidebar"},"modules/memory/examples/index":{"id":"modules/memory/examples/index","title":"Examples: Memory","description":"","sidebar":"sidebar"},"modules/model_io/models/chat/index":{"id":"modules/model_io/models/chat/index","title":"Getting Started: Chat Models","description":"Conceptual Guide","sidebar":"sidebar"},"modules/model_io/models/chat/integrations":{"id":"modules/model_io/models/chat/integrations","title":"Integrations: Chat Models","description":"LangChain offers a number of Chat Models implementations that integrate with various model providers. These are:","sidebar":"sidebar"},"modules/model_io/models/embeddings/index":{"id":"modules/model_io/models/embeddings/index","title":"Getting Started: Embeddings","description":"Conceptual Guide","sidebar":"sidebar"},"modules/model_io/models/embeddings/integrations":{"id":"modules/model_io/models/embeddings/integrations","title":"Integrations: Embeddings","description":"LangChain offers a number of Embeddings implementations that integrate with various model providers. These are:","sidebar":"sidebar"},"modules/model_io/models/index":{"id":"modules/model_io/models/index","title":"Models","description":"Conceptual Guide","sidebar":"sidebar"},"modules/model_io/models/llms/index":{"id":"modules/model_io/models/llms/index","title":"LLMs","description":"Conceptual Guide","sidebar":"sidebar"},"modules/model_io/models/llms/Integrations/huggingface":{"id":"modules/model_io/models/llms/Integrations/huggingface","title":"Hugging Face","description":"Overview","sidebar":"sidebar"},"modules/model_io/models/llms/Integrations/local":{"id":"modules/model_io/models/llms/Integrations/local","title":"Local","description":"","sidebar":"sidebar"},"modules/model_io/models/llms/Integrations/openai":{"id":"modules/model_io/models/llms/Integrations/openai","title":"OpenAI","description":"OpenAI offers a spectrum of models with different levels of power suitable for different tasks.","sidebar":"sidebar"},"modules/model_io/models/llms/Integrations/vertexai":{"id":"modules/model_io/models/llms/Integrations/vertexai","title":"Vertex AI","description":"To use the Vertex AI LLM you need to set the google project ID.","sidebar":"sidebar"},"modules/model_io/output_parsers/index":{"id":"modules/model_io/output_parsers/index","title":"Output Parsers","description":"Conceptual Guide","sidebar":"sidebar"},"modules/model_io/prompts/index":{"id":"modules/model_io/prompts/index","title":"Prompts","description":"Conceptual Guide","sidebar":"sidebar"},"modules/model_io/prompts/prompt_templates/index":{"id":"modules/model_io/prompts/prompt_templates/index","title":"Prompt templates","description":"Conceptual Guide","sidebar":"sidebar"},"modules/model_io/prompts/prompt_templates/partial_values":{"id":"modules/model_io/prompts/prompt_templates/partial_values","title":"Partial Values","description":"It can often make sense to \\"partial\\" a prompt template - eg pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.","sidebar":"sidebar"}}}')}}]);